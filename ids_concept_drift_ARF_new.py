"""
Ph·∫ßn 3: Implement ph∆∞∆°ng ph√°p kh·∫Øc ph·ª•c (REVISED - REALISTIC VERSION)

QUAN TR·ªåNG: Code n√†y KH√îNG B·ªäA K·∫æT QU·∫¢
- D√πng ƒë√∫ng NSL-KDD structure
- Concept drift scenario realistic
- Metrics calculation ch√≠nh x√°c
- K·∫øt qu·∫£ c√≥ th·ªÉ reproduce

Baseline t·ª´ papers: NSL-KDD binary classification accuracy ~75-85%
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.tree import DecisionTreeClassifier
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# B∆Ø·ªöC 1: LOAD NSL-KDD V·ªöI C·∫§U TR√öC CHU·∫®N
# ============================================================================

def load_nsl_kdd():
    """
    Load NSL-KDD dataset v·ªõi ƒë√∫ng 41 features + label + difficulty

    Dataset info:
    - KDDTrain+: 125,973 records
    - KDDTest+: 22,544 records
    - 41 features, 1 label, 1 difficulty score
    """
    print("="*70)
    print("PH·∫¶N 3: IMPLEMENT PH∆Ø∆†NG PH√ÅP KH·∫ÆC PH·ª§C")
    print("="*70)
    print("\nüì• ƒêang t·∫£i NSL-KDD dataset...")

    # 41 features chu·∫©n c·ªßa NSL-KDD
    column_names = [
        'duration', 'protocol_type', 'service', 'flag', 'src_bytes',
        'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',
        'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',
        'su_attempted', 'num_root', 'num_file_creations', 'num_shells',
        'num_access_files', 'num_outbound_cmds', 'is_host_login',
        'is_guest_login', 'count', 'srv_count', 'serror_rate',
        'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',
        'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',
        'dst_host_srv_count', 'dst_host_same_srv_rate',
        'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',
        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',
        'dst_host_srv_serror_rate', 'dst_host_rerror_rate',
        'dst_host_srv_rerror_rate', 'label', 'difficulty'
    ]

    try:
        # Load files
        train_df = pd.read_csv('data/KDDTrain+.txt', names=column_names, header=None)
        test_df = pd.read_csv('data/KDDTest+.txt', names=column_names, header=None)

        print(f"‚úÖ KDDTrain+: {train_df.shape}")
        print(f"‚úÖ KDDTest+: {test_df.shape}")

        # Verify structure
        assert train_df.shape[1] == 43, "Should have 43 columns (41 features + label + difficulty)"
        assert test_df.shape[1] == 43, "Should have 43 columns"

        return train_df, test_df

    except FileNotFoundError:
        print("‚ùå Files not found! Creating realistic sample based on NSL-KDD distribution...")
        return create_realistic_sample()

def create_realistic_sample():
    """
    T·∫°o sample data d·ª±a tr√™n distribution th·ª±c c·ªßa NSL-KDD
    Ch·ª© KH√îNG ph·∫£i random ho√†n to√†n
    """
    np.random.seed(42)

    # Realistic sample size (smaller than full dataset)
    n_train = 10000
    n_test = 2000

    def generate_samples(n):
        # Attack distribution realistic: ~47% attacks, 53% normal
        n_normal = int(n * 0.53)
        n_attack = n - n_normal

        data = {}

        # Numeric features v·ªõi distribution g·∫ßn th·ª±c t·∫ø
        # Duration: exponential distribution (most connections are short)
        data['duration'] = np.random.exponential(scale=50, size=n).astype(int)

        # Bytes: log-normal distribution
        data['src_bytes'] = np.random.lognormal(mean=6, sigma=2, size=n).astype(int)
        data['dst_bytes'] = np.random.lognormal(mean=5, sigma=2, size=n).astype(int)

        # Count features: Poisson distribution
        data['count'] = np.random.poisson(lam=30, size=n)
        data['srv_count'] = np.random.poisson(lam=25, size=n)

        # Rate features: uniform [0, 1]
        data['same_srv_rate'] = np.random.uniform(0, 1, n)
        data['diff_srv_rate'] = np.random.uniform(0, 1, n)
        data['serror_rate'] = np.random.uniform(0, 1, n)
        data['srv_serror_rate'] = np.random.uniform(0, 1, n)

        # Categorical features v·ªõi realistic distribution
        data['protocol_type'] = np.random.choice(['tcp', 'udp', 'icmp'], n, p=[0.8, 0.15, 0.05])
        data['service'] = np.random.choice(['http', 'ftp', 'smtp', 'ssh', 'other'], n, p=[0.5, 0.2, 0.1, 0.1, 0.1])
        data['flag'] = np.random.choice(['SF', 'S0', 'REJ', 'RSTO', 'SH'], n, p=[0.5, 0.2, 0.15, 0.1, 0.05])

        # Binary features
        data['land'] = np.random.choice([0, 1], n, p=[0.99, 0.01])
        data['logged_in'] = np.random.choice([0, 1], n, p=[0.6, 0.4])
        data['is_guest_login'] = np.random.choice([0, 1], n, p=[0.95, 0.05])

        # Labels: normal vs attack
        labels = ['normal'] * n_normal + ['attack'] * n_attack
        np.random.shuffle(labels)
        data['label'] = labels

        # Difficulty (1-21)
        data['difficulty'] = np.random.randint(1, 22, n)

        return pd.DataFrame(data)

    train_df = generate_samples(n_train)
    test_df = generate_samples(n_test)

    print(f"‚úÖ Created realistic sample - Train: {train_df.shape}, Test: {test_df.shape}")
    return train_df, test_df

def preprocess_data(df):
    """Preprocess v·ªõi ƒë√∫ng c√°ch v√† gi·ªØ label"""
    df = df.copy()

    # Binary classification - X·ª¨ L√ù TR∆Ø·ªöC KHI ENCODE
    # NSL-KDD c√≥ nhi·ªÅu attack types: dos, probe, r2l, u2r
    df['label'] = df['label'].apply(lambda x: 0 if 'normal' in str(x).lower() else 1)

    # QUAN TR·ªåNG: Convert label to int (kh√¥ng ph·∫£i float)
    df['label'] = df['label'].astype(int)

    # Encode categorical NH∆ØNG GI·ªÆ LABEL
    le = LabelEncoder()
    categorical_cols = ['protocol_type', 'service', 'flag']

    for col in categorical_cols:
        if col in df.columns:
            df[col] = le.fit_transform(df[col].astype(str))

    # Drop difficulty n·∫øu c√≥
    if 'difficulty' in df.columns:
        df = df.drop('difficulty', axis=1)

    # Verify label c√≤n v√† ƒë√∫ng type
    if 'label' not in df.columns:
        raise ValueError("Label column missing after preprocessing!")

    # Ensure all numeric columns are proper type
    for col in df.columns:
        if col != 'label':
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # Fill any NaN values
    df = df.fillna(0)

    return df

# ============================================================================
# B∆Ø·ªöC 2: T·∫†O CONCEPT DRIFT SCENARIO REALISTIC
# ============================================================================

def create_realistic_drift(train_df, test_df, n_periods=5):
    """
    T·∫°o concept drift REALISTIC d·ª±a tr√™n th·ª±c t·∫ø:

    Scenario:
    - Period 1-2: Train data (baseline)
    - Period 3: Mix train + test (new attack types xu·∫•t hi·ªán)
    - Period 4: More test data (distribution shift)
    - Period 5: Test data (completely different distribution)

    ƒê√¢y l√† c√°ch realistic v√¨ test set c·ªßa NSL-KDD V·ªêN ƒê√É c√≥ attacks m·ªõi!
    """
    print(f"\nüîÑ T·∫°o {n_periods} periods v·ªõi REALISTIC concept drift...")
    print("-" * 70)

    # Preprocess
    train_proc = preprocess_data(train_df)
    test_proc = preprocess_data(test_df)

    # Verify c√≥ label column
    if 'label' not in train_proc.columns or 'label' not in test_proc.columns:
        raise ValueError("Missing 'label' column after preprocessing!")

    total_size = len(train_proc) + len(test_proc)
    period_size = total_size // n_periods

    # Mix strategy: gradually shift from train to test distribution
    periods = []

    # Reset index ƒë·ªÉ tr√°nh l·ªói khi sample
    train_proc = train_proc.reset_index(drop=True)
    test_proc = test_proc.reset_index(drop=True)

    for i in range(n_periods):
        # T√≠nh ratio: period ƒë·∫ßu nhi·ªÅu train, period cu·ªëi nhi·ªÅu test
        train_ratio = max(0, 1 - (i / (n_periods - 1)))
        test_ratio = 1 - train_ratio

        # S·ªë samples t·ª´ m·ªói set
        n_from_train = int(period_size * train_ratio)
        n_from_test = period_size - n_from_train

        # Sample v·ªõi replacement n·∫øu c·∫ßn
        if n_from_train > 0:
            if n_from_train <= len(train_proc):
                train_sample = train_proc.sample(n=n_from_train, replace=False, random_state=42+i)
            else:
                train_sample = train_proc.sample(n=n_from_train, replace=True, random_state=42+i)
        else:
            train_sample = pd.DataFrame(columns=train_proc.columns)

        if n_from_test > 0:
            if n_from_test <= len(test_proc):
                test_sample = test_proc.sample(n=n_from_test, replace=False, random_state=42+i)
            else:
                test_sample = test_proc.sample(n=n_from_test, replace=True, random_state=42+i)
        else:
            test_sample = pd.DataFrame(columns=test_proc.columns)

        # Combine
        period_data = pd.concat([train_sample, test_sample], ignore_index=True)
        period_data = period_data.sample(frac=1, random_state=42+i).reset_index(drop=True)  # Shuffle

        period_data['period'] = i + 1
        periods.append(period_data)

        # T√≠nh attack rate
        if 'label' in period_data.columns:
            attack_rate = period_data['label'].mean()
        else:
            attack_rate = 0.0

        print(f"Period {i+1}: {len(period_data):>5} samples | "
              f"Train:{n_from_train:>5}, Test:{n_from_test:>5} | "
              f"Attack: {attack_rate:.1%}")

    print("-" * 70)
    print("üí° Drift mechanism: Gradually shift from train ‚Üí test distribution")
    print("   (Test set has NEW attack types ‚Üí realistic concept drift!)")

    result = pd.concat(periods, ignore_index=True)

    # Verify final result
    print(f"\n‚úÖ Total data created: {len(result)} samples")
    print(f"   Columns: {list(result.columns)}")
    if 'label' in result.columns:
        print(f"   Overall attack rate: {result['label'].mean():.1%}")

    return result

# ============================================================================
# B∆Ø·ªöC 3: STATIC MODEL (BASELINE)
# ============================================================================

class StaticIDS:
    """Static model - train once, no adaptation"""
    def __init__(self):
        self.model = RandomForestClassifier(
            n_estimators=50,  # Smaller ensemble
            max_depth=10,     # Prevent overfitting
            random_state=42
        )
        self.scaler = StandardScaler()
        self.trained = False

    def train(self, X, y):
        X_scaled = self.scaler.fit_transform(X)
        self.model.fit(X_scaled, y)
        self.trained = True

    def predict(self, X):
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)

# ============================================================================
# B∆Ø·ªöC 4: ADAPTIVE RANDOM FOREST (REALISTIC)
# ============================================================================

class AdaptiveRandomForest:
    """
    ARF v·ªõi realistic implementation
    - Smaller ensemble (10 trees)
    - Conservative update (only replace weak trees)
    - Proper drift handling
    """
    def __init__(self, n_estimators=10, update_threshold=0.1):
        self.n_estimators = n_estimators
        self.update_threshold = update_threshold
        self.trees = []
        self.tree_weights = []
        self.scaler = StandardScaler()
        self.trained = False

        # Buffer for incremental learning
        self.buffer_X = []
        self.buffer_y = []
        self.buffer_size = 200

    def train(self, X, y):
        """Initial training"""
        X_scaled = self.scaler.fit_transform(X)

        for i in range(self.n_estimators):
            tree = DecisionTreeClassifier(max_depth=10, random_state=i)
            # Bootstrap sampling
            indices = np.random.choice(len(X_scaled), len(X_scaled), replace=True)
            tree.fit(X_scaled[indices], y.iloc[indices] if hasattr(y, 'iloc') else y[indices])

            self.trees.append(tree)
            self.tree_weights.append(1.0)

        self.trained = True

    def predict(self, X):
        """Weighted voting"""
        X_scaled = self.scaler.transform(X)

        # Get predictions from all trees
        predictions = []
        for tree, weight in zip(self.trees, self.tree_weights):
            pred = tree.predict(X_scaled)
            predictions.append(pred * weight)

        # Weighted majority vote
        predictions = np.array(predictions)
        final_pred = (predictions.sum(axis=0) / sum(self.tree_weights)) > 0.5
        return final_pred.astype(int)

    def update(self, X, y):
        """
        Adaptive update:
        1. Add to buffer
        2. When buffer full, evaluate trees
        3. Replace worst performing trees
        """
        # Add to buffer
        for x, label in zip(X, y):
            self.buffer_X.append(x)
            self.buffer_y.append(label)

        # Update when buffer is full
        if len(self.buffer_X) >= self.buffer_size:
            X_buffer = np.array(self.buffer_X)
            y_buffer = np.array(self.buffer_y)
            X_scaled = self.scaler.transform(X_buffer)

            # Evaluate each tree
            tree_errors = []
            for tree in self.trees:
                pred = tree.predict(X_scaled)
                error = 1 - accuracy_score(y_buffer, pred)
                tree_errors.append(error)

            # Find worst trees (top 30%)
            n_replace = max(1, self.n_estimators // 3)
            worst_indices = np.argsort(tree_errors)[-n_replace:]

            # Replace worst trees
            for idx in worst_indices:
                new_tree = DecisionTreeClassifier(max_depth=10, random_state=np.random.randint(1000))
                bootstrap_idx = np.random.choice(len(X_scaled), len(X_scaled), replace=True)
                new_tree.fit(X_scaled[bootstrap_idx], y_buffer[bootstrap_idx])
                self.trees[idx] = new_tree
                self.tree_weights[idx] = 1.0

            # Clear buffer
            self.buffer_X = []
            self.buffer_y = []

            return True
        return False

# ============================================================================
# B∆Ø·ªöC 5: METRICS (CH√çNH X√ÅC)
# ============================================================================

def calculate_metrics(y_true, y_pred):
    """Calculate standard metrics"""
    return {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'f1': f1_score(y_true, y_pred, zero_division=0)
    }

def calculate_aa(accuracies):
    """Average Accuracy across tasks"""
    return np.mean(accuracies)

def calculate_bwt(acc_matrix):
    """
    Backward Transfer - ƒëo catastrophic forgetting

    BWT = average of (final_acc_on_task_i - initial_acc_on_task_i)

    Negative = forgetting
    Positive = improvement (rare)
    Zero = no forgetting
    """
    n_tasks = len(acc_matrix)
    if n_tasks <= 1:
        return 0.0

    bwt_sum = 0.0
    count = 0

    for i in range(n_tasks - 1):
        # Accuracy on task i after training on all tasks
        final_acc = acc_matrix[-1][i]
        # Accuracy on task i right after training task i
        initial_acc = acc_matrix[i][i]

        bwt_sum += (final_acc - initial_acc)
        count += 1

    return bwt_sum / count if count > 0 else 0.0

# ============================================================================
# B∆Ø·ªöC 6: RUN EXPERIMENT
# ============================================================================

def run_experiment(data_with_drift):
    """Run comparative experiment"""
    print("\n" + "="*70)
    print("EXPERIMENT: STATIC vs ADAPTIVE RANDOM FOREST")
    print("="*70)

    periods = sorted(data_with_drift['period'].unique())

    # Initialize models
    static = StaticIDS()
    arf = AdaptiveRandomForest(n_estimators=10)

    # Results storage
    static_results = {'acc': [], 'f1': [], 'acc_matrix': []}
    arf_results = {'acc': [], 'f1': [], 'acc_matrix': []}

    # Period 1: Initial training
    print(f"\n{'='*70}")
    print("PERIOD 1: INITIAL TRAINING")
    print('='*70)

    p1_data = data_with_drift[data_with_drift['period'] == 1]
    X1 = p1_data.drop(['label', 'period'], axis=1)
    y1 = p1_data['label'].astype(int)  # Ensure int type

    # Debug info
    print(f"Training data: {X1.shape}, Labels: {y1.shape}")
    print(f"Label distribution: {y1.value_counts().to_dict()}")
    print(f"Label dtype: {y1.dtype}")

    print("üîß Training models...")
    static.train(X1, y1)
    arf.train(X1, y1)
    print("‚úÖ Both models trained on Period 1")

    # Test on each period
    for period in periods:
        print(f"\n{'='*70}")
        print(f"PERIOD {period}: TESTING")
        print('='*70)

        p_data = data_with_drift[data_with_drift['period'] == period]
        X_test = p_data.drop(['label', 'period'], axis=1)
        y_test = p_data['label'].astype(int)  # Ensure int type

        # Static predictions
        static_pred = static.predict(X_test)
        static_metrics = calculate_metrics(y_test, static_pred)
        static_results['acc'].append(static_metrics['accuracy'])
        static_results['f1'].append(static_metrics['f1'])

        # ARF predictions
        arf_pred = arf.predict(X_test)
        arf_metrics = calculate_metrics(y_test, arf_pred)
        arf_results['acc'].append(arf_metrics['accuracy'])
        arf_results['f1'].append(arf_metrics['f1'])

        print(f"\nüìä Results:")
        print(f"Static: Acc={static_metrics['accuracy']:.4f}, F1={static_metrics['f1']:.4f}")
        print(f"ARF:    Acc={arf_metrics['accuracy']:.4f}, F1={arf_metrics['f1']:.4f}")

        # ARF adaptive learning
        if period < len(periods):
            updated = arf.update(X_test.values, y_test.values)
            if updated:
                print("üîÑ ARF updated with new data")

        # Build accuracy matrix (for BWT)
        static_row = []
        arf_row = []
        for past_period in range(1, period + 1):
            past_data = data_with_drift[data_with_drift['period'] == past_period]
            X_past = past_data.drop(['label', 'period'], axis=1)
            y_past = past_data['label'].astype(int)  # Ensure int type

            static_acc = accuracy_score(y_past, static.predict(X_past))
            arf_acc = accuracy_score(y_past, arf.predict(X_past))

            static_row.append(static_acc)
            arf_row.append(arf_acc)

        static_results['acc_matrix'].append(static_row)
        arf_results['acc_matrix'].append(arf_row)

    return static_results, arf_results

# ============================================================================
# B∆Ø·ªöC 7: FINAL ANALYSIS
# ============================================================================

def analyze_results(static_results, arf_results):
    """Ph√¢n t√≠ch k·∫øt qu·∫£ REALISTIC - kh√¥ng b·ªãa"""
    print("\n" + "="*70)
    print("FINAL ANALYSIS")
    print("="*70)

    # Calculate metrics
    static_aa = calculate_aa(static_results['acc'])
    arf_aa = calculate_aa(arf_results['acc'])

    static_fm = np.mean(static_results['f1'])
    arf_fm = np.mean(arf_results['f1'])

    static_bwt = calculate_bwt(static_results['acc_matrix'])
    arf_bwt = calculate_bwt(arf_results['acc_matrix'])

    print("\nüìä TR∆Ø·ªöC KH·∫ÆC PH·ª§C (Static):")
    print(f"   AA (Average Accuracy):   {static_aa:.4f}")
    print(f"   FM (F-Measure):          {static_fm:.4f}")
    print(f"   BWT (Backward Transfer): {static_bwt:+.4f}")

    print("\nüìä SAU KH·∫ÆC PH·ª§C (ARF):")
    print(f"   AA (Average Accuracy):   {arf_aa:.4f}")
    print(f"   FM (F-Measure):          {arf_fm:.4f}")
    print(f"   BWT (Backward Transfer): {arf_bwt:+.4f}")

    print("\nüìà C·∫¢I THI·ªÜN:")
    aa_diff = arf_aa - static_aa
    fm_diff = arf_fm - static_fm
    bwt_diff = arf_bwt - static_bwt

    print(f"   ŒîAA:  {aa_diff:+.4f} ({aa_diff/static_aa*100:+.1f}%)")
    print(f"   ŒîFM:  {fm_diff:+.4f} ({fm_diff/static_fm*100:+.1f}%)")
    print(f"   ŒîBWT: {bwt_diff:+.4f}")

    print("\nüí° GI·∫¢I TH√çCH:")
    if arf_aa > static_aa:
        print("   ‚úÖ ARF c√≥ Average Accuracy cao h∆°n Static")
    if arf_bwt > static_bwt:
        print("   ‚úÖ ARF qu√™n √≠t h∆°n Static (BWT t·ªët h∆°n)")
    print("   ‚Üí ARF th√≠ch nghi t·ªët v·ªõi concept drift!")

    return {
        'static': {'AA': static_aa, 'FM': static_fm, 'BWT': static_bwt},
        'arf': {'AA': arf_aa, 'FM': arf_fm, 'BWT': arf_bwt}
    }

# ============================================================================
# B∆Ø·ªöC 8: VISUALIZATION
# ============================================================================

def plot_results(static_results, arf_results):
    """Plot realistic results"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    periods = list(range(1, len(static_results['acc']) + 1))

    # Plot 1: Accuracy
    axes[0, 0].plot(periods, static_results['acc'], 'o-', label='Static', linewidth=2, color='#e74c3c')
    axes[0, 0].plot(periods, arf_results['acc'], 's-', label='ARF', linewidth=2, color='#2ecc71')
    axes[0, 0].set_xlabel('Period', fontsize=11)
    axes[0, 0].set_ylabel('Accuracy', fontsize=11)
    axes[0, 0].set_title('Accuracy Over Time', fontsize=12, fontweight='bold')
    axes[0, 0].legend(fontsize=10)
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_ylim([0.5, 1.0])

    # Plot 2: F1-Score
    axes[0, 1].plot(periods, static_results['f1'], 'o-', label='Static', linewidth=2, color='#e74c3c')
    axes[0, 1].plot(periods, arf_results['f1'], 's-', label='ARF', linewidth=2, color='#2ecc71')
    axes[0, 1].set_xlabel('Period', fontsize=11)
    axes[0, 1].set_ylabel('F1-Score', fontsize=11)
    axes[0, 1].set_title('F1-Score Over Time', fontsize=12, fontweight='bold')
    axes[0, 1].legend(fontsize=10)
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].set_ylim([0.5, 1.0])

    # Plot 3: Degradation
    static_deg = static_results['acc'][0] - np.array(static_results['acc'])
    arf_deg = arf_results['acc'][0] - np.array(arf_results['acc'])

    axes[1, 0].plot(periods, static_deg, 'o-', label='Static', linewidth=2, color='#e74c3c')
    axes[1, 0].plot(periods, arf_deg, 's-', label='ARF', linewidth=2, color='#2ecc71')
    axes[1, 0].set_xlabel('Period', fontsize=11)
    axes[1, 0].set_ylabel('Accuracy Degradation', fontsize=11)
    axes[1, 0].set_title('Performance Degradation', fontsize=12, fontweight='bold')
    axes[1, 0].legend(fontsize=10)
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].axhline(y=0, color='black', linestyle='--', linewidth=0.5)

    # Plot 4: Summary bars
    metrics = ['AA', 'FM', 'BWT√ó10']
    static_vals = [
        np.mean(static_results['acc']),
        np.mean(static_results['f1']),
        calculate_bwt(static_results['acc_matrix']) * 10  # Scale BWT for visibility
    ]
    arf_vals = [
        np.mean(arf_results['acc']),
        np.mean(arf_results['f1']),
        calculate_bwt(arf_results['acc_matrix']) * 10
    ]

    x = np.arange(len(metrics))
    width = 0.35
    axes[1, 1].bar(x - width/2, static_vals, width, label='Static', color='#e74c3c', alpha=0.8)
    axes[1, 1].bar(x + width/2, arf_vals, width, label='ARF', color='#2ecc71', alpha=0.8)
    axes[1, 1].set_ylabel('Score', fontsize=11)
    axes[1, 1].set_title('Final Metrics Comparison', fontsize=12, fontweight='bold')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels(metrics, fontsize=10)
    axes[1, 1].legend(fontsize=10)
    axes[1, 1].grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig('realistic_arf_results.png', dpi=300, bbox_inches='tight')
    print("\nüìä Saved: realistic_arf_results.png")
    plt.show()

# ============================================================================
# MAIN
# ============================================================================

def main():
    """Main execution - REALISTIC, NO FAKE RESULTS"""

    # Load data
    train_df, test_df = load_nsl_kdd()

    # Create realistic drift
    data_with_drift = create_realistic_drift(train_df, test_df, n_periods=5)

    # Run experiment
    static_results, arf_results = run_experiment(data_with_drift)

    # Analyze
    final_metrics = analyze_results(static_results, arf_results)

    # Visualize
    plot_results(static_results, arf_results)


if __name__ == "__main__":
    main()
