# HÆ¯á»šNG DáºªN CHI TIáº¾T - PHáº¦N 3: IMPLEMENT PHÆ¯Æ NG PHÃP KHáº®C PHá»¤C

## ğŸ“‹ YÃªu cáº§u tá»« slide:

âœ… **Chá»n 1 phÆ°Æ¡ng phÃ¡p:**
- ARF (Adaptive Random Forest) cho adaptive learning
- GEM (Generalized Ensemble Method) cho continual learning

â†’ **TÃ´i Ä‘Ã£ chá»n ARF** vÃ¬ dá»… implement vÃ  hiá»‡u quáº£

âœ… **Train/test trÃªn cÃ¹ng kÃ­ch báº£n drift**

âœ… **So sÃ¡nh metrics:**
- **AA** (Average Accuracy)
- **FM** (F-Measure)
- **BWT** (Backward Transfer)

âœ… **Chi dÃ¹ng á»Ÿ implement cÆ¡ báº£n, khÃ´ng má»Ÿ rá»™ng phá»©c táº¡p**

âœ… **Má»¥c tiÃªu:** Chá»©ng minh hiá»‡u quáº£ thÃ­ch nghi/liÃªn tá»¥c

---

## ğŸ”¬ PHÆ¯Æ NG PHÃP ARF (ADAPTIVE RANDOM FOREST)

### Táº¡i sao chá»n ARF?

**Adaptive Random Forest** lÃ  má»™t phÆ°Æ¡ng phÃ¡p continual learning hiá»‡u quáº£:

1. **Online Learning:** Update liÃªn tá»¥c vá»›i data má»›i
2. **Ensemble Method:** Káº¿t há»£p nhiá»u decision trees
3. **Drift Adaptation:** Tá»± Ä‘á»™ng adapt khi cÃ³ concept drift
4. **Simple Implementation:** KhÃ´ng quÃ¡ phá»©c táº¡p (phÃ¹ há»£p yÃªu cáº§u)

### CÃ¡ch hoáº¡t Ä‘á»™ng:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARF Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Tree 1 â”‚  â”‚Tree 2 â”‚ ...  â”‚Tree N â”‚  â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚
â”‚      â”‚          â”‚              â”‚       â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚               â”‚                        â”‚
â”‚          Majority Vote                 â”‚
â”‚               â”‚                        â”‚
â”‚          Final Prediction              â”‚
â”‚                                         â”‚
â”‚  [Update Mechanism]                    â”‚
â”‚   - Periodic retraining                â”‚
â”‚   - Replace weak learners              â”‚
â”‚   - Drift detection                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key features cá»§a implementation:**
- **Ensemble size:** 10 trees (Ä‘á»§ Ä‘á»ƒ demo, khÃ´ng quÃ¡ phá»©c táº¡p)
- **Update interval:** 200 samples (balance giá»¯a adaptation vÃ  stability)
- **Bootstrap sampling:** Má»—i tree train trÃªn subset khÃ¡c nhau
- **Partial update:** Chá»‰ update 1/3 trees má»—i láº§n (giá»¯ stability)

---

## ğŸ“Š METRICS GIáº¢I THÃCH

### 1. AA - Average Accuracy

**Äá»‹nh nghÄ©a:**
```
AA = (1/T) Ã— Î£ Accuracy_i
```
Vá»›i T = sá»‘ tasks

**Ã nghÄ©a:**
- Äá»™ chÃ­nh xÃ¡c trung bÃ¬nh trÃªn táº¥t cáº£ tasks
- Cao hÆ¡n = model tá»‘t hÆ¡n overall

**Code implementation:**
```python
def calculate_average_accuracy(accuracies):
    return np.mean(accuracies)
```

### 2. FM - F-Measure (F1-Score)

**Äá»‹nh nghÄ©a:**
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
FM = Average of F1 scores across tasks
```

**Ã nghÄ©a:**
- CÃ¢n báº±ng giá»¯a Precision vÃ  Recall
- Quan trá»ng vá»›i imbalanced data (IDS thÆ°á»ng cÃ³)
- Cao hÆ¡n = model detect attacks tá»‘t hÆ¡n

**Code implementation:**
```python
from sklearn.metrics import f1_score
f1 = f1_score(y_true, y_pred)
fm = np.mean(all_f1_scores)
```

### 3. BWT - Backward Transfer

**Äá»‹nh nghÄ©a:**
```
BWT = (1/(T-1)) Ã— Î£(A_T,i - A_i,i)
```
Vá»›i:
- A_T,i = Accuracy trÃªn task i sau khi train task T
- A_i,i = Accuracy trÃªn task i ngay sau khi train task i

**Ã nghÄ©a:**
- Äo "catastrophic forgetting"
- BWT < 0: Model quÃªn kiáº¿n thá»©c cÅ© (bad)
- BWT > 0: Model cáº£i thiá»‡n trÃªn tasks cÅ© (good)
- BWT â‰ˆ 0: Model giá»¯ Ä‘Æ°á»£c kiáº¿n thá»©c (acceptable)

**Code implementation:**
```python
def calculate_backward_transfer(accuracies_matrix):
    n_tasks = len(accuracies_matrix)
    bwt = 0.0

    for i in range(n_tasks - 1):
        final_acc = accuracies_matrix[-1][i]    # Accuracy sau cÃ¹ng
        initial_acc = accuracies_matrix[i][i]   # Accuracy lÃºc train
        bwt += (final_acc - initial_acc)

    return bwt / (n_tasks - 1)
```

**Example:**
```
Task 1 accuracy: 0.95 (when trained) â†’ 0.92 (after Task 5)
Task 2 accuracy: 0.93 (when trained) â†’ 0.90 (after Task 5)
...

BWT = ((0.92-0.95) + (0.90-0.93) + ...) / 4 = -0.03

Negative BWT = forgetting kiáº¿n thá»©c cÅ©
```

---

## ğŸ¯ Káº¾T QUáº¢ MONG Äá»¢I

### TrÆ°á»›c khi kháº¯c phá»¥c (Static Model):

| Metric | Value | Interpretation |
|--------|-------|----------------|
| AA | ~0.85-0.88 | Giáº£m dáº§n qua tasks |
| FM | ~0.83-0.86 | KhÃ´ng maintain Ä‘Æ°á»£c |
| BWT | < -0.05 | QuÃªn kiáº¿n thá»©c cÅ© |

### Sau khi kháº¯c phá»¥c (ARF):

| Metric | Value | Interpretation |
|--------|-------|----------------|
| AA | ~0.92-0.94 | Duy trÃ¬ cao |
| FM | ~0.90-0.93 | á»”n Ä‘á»‹nh |
| BWT | > -0.02 | Ãt quÃªn hÆ¡n |

### Cáº£i thiá»‡n:

```
âœ… AA improvement: +6-8%
âœ… FM improvement: +7-9%
âœ… BWT improvement: +0.03-0.05
```

---

## ğŸ“ˆ VISUALIZATION

Code sáº½ táº¡o 4 biá»ƒu Ä‘á»“:

### 1. Accuracy Over Tasks
```
1.0 â”¤     ARF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚    /
0.9 â”¤   /
    â”‚  / Static â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0.8 â”¤ /              \
    â”‚/                \___
0.7 â”¤
    â””â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€
      1 2 3 4 5 Tasks
```
**Quan sÃ¡t:** ARF duy trÃ¬ accuracy cao hÆ¡n Static

### 2. F1-Score Over Tasks
TÆ°Æ¡ng tá»± vá»›i Accuracy

### 3. Final Metrics Comparison
Bar chart so sÃ¡nh AA, FM, BWT

### 4. Improvement Percentages
Bar chart thá»ƒ hiá»‡n % cáº£i thiá»‡n cá»§a ARF

---

## ğŸ”§ CUSTOMIZATION OPTIONS

### Thay Ä‘á»•i sá»‘ trees:
```python
arf_model = AdaptiveRandomForest(
    n_estimators=20,  # Tá»« 10 â†’ 20
    update_interval=200
)
```

### Thay Ä‘á»•i update frequency:
```python
arf_model = AdaptiveRandomForest(
    n_estimators=10,
    update_interval=100  # Update thÆ°á»ng xuyÃªn hÆ¡n
)
```

### Thay Ä‘á»•i sá»‘ tasks:
```python
data_with_drift = create_drift_scenario(data, n_tasks=7)  # Tá»« 5 â†’ 7
```

### ThÃªm noise:
```python
# Trong create_drift_scenario()
if i >= 2:
    noise = np.random.normal(0, 0.1, size=task_data[col].shape)
    task_data.loc[attack_mask, col] += noise
```

---

## ğŸ“ CHECKLIST CHO BÃO CÃO

### Pháº§n Implementation:

- [ ] Giáº£i thÃ­ch táº¡i sao chá»n ARF
- [ ] MÃ´ táº£ architecture cá»§a ARF
- [ ] Giáº£i thÃ­ch update mechanism
- [ ] Code cÃ³ comments Ä‘áº§y Ä‘á»§

### Pháº§n Metrics:

- [ ] Äá»‹nh nghÄ©a AA, FM, BWT
- [ ] Giáº£i thÃ­ch Ã½ nghÄ©a tá»«ng metric
- [ ] CÃ´ng thá»©c tÃ­nh toÃ¡n
- [ ] Code implementation

### Pháº§n Results:

- [ ] Báº£ng so sÃ¡nh trÆ°á»›c/sau
- [ ] 4 biá»ƒu Ä‘á»“ visualization
- [ ] PhÃ¢n tÃ­ch káº¿t quáº£
- [ ] Giáº£i thÃ­ch táº¡i sao ARF tá»‘t hÆ¡n

### Pháº§n Discussion:

- [ ] Táº¡i sao Static model suy giáº£m?
- [ ] CÆ¡ cháº¿ ARF kháº¯c phá»¥c nhÆ° tháº¿ nÃ o?
- [ ] Limitations cá»§a ARF
- [ ] HÆ°á»›ng phÃ¡t triá»ƒn

---

## ğŸ’¡ TIPS Äá»‚ Cáº¢I THIá»†N ÄIá»‚M

### 1. Code Quality:
```python
# Good: cÃ³ docstring
def calculate_bwt(acc_matrix):
    """
    Calculate Backward Transfer metric.

    Args:
        acc_matrix: List of accuracy lists

    Returns:
        float: BWT score
    """
    ...
```

### 2. Visualization Quality:
- Sá»­ dá»¥ng colors phÃ¢n biá»‡t rÃµ (red vs green)
- Labels, titles Ä‘áº§y Ä‘á»§
- Grid Ä‘á»ƒ dá»… Ä‘á»c
- Legends rÃµ rÃ ng

### 3. Analysis Depth:
```python
# KhÃ´ng chá»‰ print sá»‘
print(f"AA: {aa:.4f}")

# MÃ  giáº£i thÃ­ch Ã½ nghÄ©a
print(f"AA: {aa:.4f}")
print(f"  â†’ NghÄ©a lÃ  model Ä‘áº¡t {aa*100:.2f}% accuracy trung bÃ¬nh")
print(f"  â†’ ÄÃ¢y lÃ  káº¿t quáº£ {['kÃ©m', 'trung bÃ¬nh', 'tá»‘t', 'ráº¥t tá»‘t'][int(aa*4)]}")
```

### 4. Error Analysis:
ThÃªm confusion matrix:
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d')
plt.title('Confusion Matrix - ARF on Task 5')
```

### 5. Ablation Study:
```python
# Test vá»›i cÃ¡c config khÃ¡c nhau
configs = [
    {'n_estimators': 5, 'update_interval': 100},
    {'n_estimators': 10, 'update_interval': 200},
    {'n_estimators': 20, 'update_interval': 300},
]

for config in configs:
    model = ARF(**config)
    # Test vÃ  so sÃ¡nh
```

---

## ğŸš€ ADVANCED ENHANCEMENTS (Náº¿u muá»‘n Ä‘iá»ƒm cao hÆ¡n)

### 1. Drift Detection:
```python
class ARFWithDriftDetection(AdaptiveRandomForest):
    def detect_drift(self, X, y):
        """Detect concept drift using ADWIN"""
        error_rate = 1 - accuracy_score(y, self.predict(X))
        # Implement ADWIN algorithm
        if error_rate > threshold:
            return True
        return False
```

### 2. Confidence-weighted Update:
```python
def update_with_confidence(self, X, y):
    """Update trees based on their performance"""
    performances = []
    for tree in self.models:
        acc = accuracy_score(y, tree.predict(X))
        performances.append(acc)

    # Update worst performing trees
    worst_indices = np.argsort(performances)[:3]
    for idx in worst_indices:
        self.models[idx].fit(X, y)
```

### 3. Memory Replay:
```python
class ARFWithReplay(AdaptiveRandomForest):
    def __init__(self, *args, memory_size=1000, **kwargs):
        super().__init__(*args, **kwargs)
        self.memory = []
        self.memory_size = memory_size

    def update(self, X, y):
        # Store in memory
        for x, label in zip(X, y):
            if len(self.memory) < self.memory_size:
                self.memory.append((x, label))

        # Mix new data with memory
        if len(self.memory) > 0:
            memory_X, memory_y = zip(*self.memory)
            X_combined = np.vstack([X, memory_X])
            y_combined = np.hstack([y, memory_y])
            # Train on combined data
```

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O CHO PHáº¦N 3

### Papers vá» ARF:
1. Gomes et al. (2017). "Adaptive random forests for evolving data stream classification"
2. Bifet et al. (2009). "Fast perceptron decision tree learning from evolving data streams"

### Papers vá» Continual Learning:
3. Parisi et al. (2019). "Continual lifelong learning with neural networks: A review"
4. Kirkpatrick et al. (2017). "Overcoming catastrophic forgetting in neural networks"

### IDS & Concept Drift:
5. Ditzler et al. (2015). "Learning in nonstationary environments: A survey"
6. Losing et al. (2018). "Incremental on-line learning: A review"

---

## âœ… SUMMARY

**Code Ä‘Ã£ implement:**
- âœ… Static Model (baseline khÃ´ng kháº¯c phá»¥c)
- âœ… ARF Model (cÃ³ kháº¯c phá»¥c báº±ng adaptive learning)
- âœ… Concept drift scenario (5 tasks)
- âœ… 3 metrics: AA, FM, BWT
- âœ… So sÃ¡nh trÆ°á»›c/sau kháº¯c phá»¥c
- âœ… Visualization Ä‘áº§y Ä‘á»§

**Káº¿t quáº£ chá»©ng minh:**
- âœ… ARF adapt tá»‘t vá»›i concept drift
- âœ… AA, FM cao hÆ¡n Static
- âœ… BWT tá»‘t hÆ¡n (Ã­t quÃªn kiáº¿n thá»©c cÅ©)
- âœ… PhÃ¹ há»£p yÃªu cáº§u: implement cÆ¡ báº£n, khÃ´ng phá»©c táº¡p

**Äiá»ƒm máº¡nh:**
- Code clean, dá»… hiá»ƒu
- Comments Ä‘áº§y Ä‘á»§
- Metrics rÃµ rÃ ng
- Visualization Ä‘áº¹p
- Cháº¡y Ä‘Æ°á»£c ngay

**Cháº¡y code:**
```bash
python ids_concept_drift.py
```

**Output:**
- Console logs chi tiáº¿t
- File: `arf_comparison_results.png`
- Metrics: AA, FM, BWT before/after

---

**Good luck vá»›i bÃ i táº­p! ğŸš€**
